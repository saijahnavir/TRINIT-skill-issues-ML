{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WXLsaisr7e4o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7px4wrH8QEv8",
        "outputId": "1bd456ba-8170-44eb-8895-485d7379dcf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YDpsMlc17FDb"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataset/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataset/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW-q8X5c1IIh",
        "outputId": "75170902-962f-4e81-aa4c-70e3cf82735f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "163/163 [==============================] - 4s 20ms/step - loss: 0.5962 - accuracy: 0.6970 - val_loss: 0.5894 - val_accuracy: 0.6921\n",
            "Epoch 2/10\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 0.5490 - accuracy: 0.7095 - val_loss: 0.5193 - val_accuracy: 0.4981\n",
            "Epoch 3/10\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 0.4084 - accuracy: 0.7281 - val_loss: 0.4780 - val_accuracy: 0.6582\n",
            "Epoch 4/10\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 0.3495 - accuracy: 0.7532 - val_loss: 0.4831 - val_accuracy: 0.7398\n",
            "Epoch 5/10\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 0.3094 - accuracy: 0.7630 - val_loss: 0.4973 - val_accuracy: 0.7182\n",
            "Epoch 6/10\n",
            "163/163 [==============================] - 2s 13ms/step - loss: 0.2782 - accuracy: 0.7482 - val_loss: 0.5295 - val_accuracy: 0.6382\n",
            "41/41 [==============================] - 0s 3ms/step\n",
            "Hamming Score: 0.8049781883500129\n",
            "F1 Score: 0.5651123938528949\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.51      0.62       481\n",
            "           1       0.59      0.15      0.25       271\n",
            "           2       0.79      0.67      0.72       389\n",
            "\n",
            "   micro avg       0.77      0.48      0.59      1141\n",
            "   macro avg       0.72      0.44      0.53      1141\n",
            "weighted avg       0.74      0.48      0.57      1141\n",
            " samples avg       0.90      0.67      0.63      1141\n",
            "\n",
            "Predictions Array:\n",
            "[[[0 0 0]]\n",
            "\n",
            " [[0 0 1]]\n",
            "\n",
            " [[0 0 0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0 0 1]]\n",
            "\n",
            " [[0 0 1]]\n",
            "\n",
            " [[0 0 0]]]\n",
            "Predictions saved to 'predictions.csv' file.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = ' '.join([word for word in word_tokenize(text) if word.isalpha() and word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Load data and preprocess text\n",
        "train_df.drop_duplicates(subset=\"Description\", keep=False, inplace=True)\n",
        "train_df['Description'] = train_df['Description'].apply(preprocess_text)\n",
        "test_df['Description'] = test_df['Description'].apply(preprocess_text)\n",
        "\n",
        "# Tokenization and padding\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_df['Description'])\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(train_df['Description'])\n",
        "X_test_sequences = tokenizer.texts_to_sequences(test_df['Description'])\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Model architecture\n",
        "embedding_size = 100\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_size, input_length=max_sequence_length),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(3, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train-test split and model training\n",
        "X = X_train_padded\n",
        "y = train_df[['Commenting', 'Ogling/Facial Expressions/Staring', 'Touching /Groping']]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Model evaluation using F1-score for multi-label classification\n",
        "predictions = model.predict(X_test)\n",
        "threshold = 0.5\n",
        "binary_predictions = (predictions > threshold).astype(int)\n",
        "f1_score = f1_score(y_test, binary_predictions, average='weighted', zero_division=1)\n",
        "\n",
        "hamming_score = 1 - hamming_loss(y_test, binary_predictions)\n",
        "print(f'Hamming Score: {hamming_score}')\n",
        "\n",
        "print(f'F1 Score: {f1_score}')\n",
        "print('Classification Report:\\n', classification_report(y_test, binary_predictions, zero_division=1))\n",
        "\n",
        "def preprocess_input_text(text):\n",
        "    text = preprocess_text(text)\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
        "    return padded_sequence\n",
        "\n",
        "\n",
        "predictions_array = []\n",
        "\n",
        "for text in test_df['Description']:\n",
        "    input_sequence = preprocess_input_text(text)\n",
        "    prediction = model.predict(input_sequence, verbose=0)\n",
        "    threshold = 0.5\n",
        "    binary_prediction = (prediction > threshold).astype(int)\n",
        "    predictions_array.append(binary_prediction)\n",
        "predictions_array = np.array(predictions_array)\n",
        "\n",
        "\n",
        "print(\"Predictions Array:\")\n",
        "print(predictions_array)\n",
        "\n",
        "# Reshape predictions_array to remove the extra dimension\n",
        "predictions_array_reshaped = predictions_array.reshape(predictions_array.shape[0], predictions_array.shape[2])\n",
        "\n",
        "# Create a DataFrame with the reshaped predictions_array\n",
        "predictions_df = pd.DataFrame(predictions_array_reshaped, columns=['Commenting', 'Ogling/Facial Expressions/Staring', 'Touching /Groping'])\n",
        "\n",
        "# Add the original 'Description' column from test_df for reference\n",
        "predictions_df['Description'] = test_df['Description'].values\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "predictions_df.to_csv('predictions.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'predictions.csv' file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J135DrE-nCN",
        "outputId": "1ff832af-09cc-41ff-ca5b-c307f2da99f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.10/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.2)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.5.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install lime shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr7ozvtACHQQ",
        "outputId": "d314eea9-9477-46a8-a73a-8d3c408731d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.2)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BemRhgbp-lse"
      },
      "outputs": [],
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt9Ub0wTAuqh",
        "outputId": "c4a2eee4-708e-428f-ce8c-dde96bc08b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 1s 4ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 3ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "157/157 [==============================] - 1s 4ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 1s 4ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 1s 4ms/step\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 1s 4ms/step\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 1s 4ms/step\n",
            "157/157 [==============================] - 1s 4ms/step\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "157/157 [==============================] - 1s 6ms/step\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "157/157 [==============================] - 1s 5ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 0s 3ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 3ms/step\n",
            "157/157 [==============================] - 0s 3ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 1s 4ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 1s 4ms/step\n",
            "157/157 [==============================] - 1s 4ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 3ms/step\n",
            "157/157 [==============================] - 0s 3ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 0s 3ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 0s 3ms/step\n",
            "157/157 [==============================] - 0s 3ms/step\n",
            "157/157 [==============================] - 1s 3ms/step\n",
            "157/157 [==============================] - 0s 2ms/step\n",
            "Explanations for the first 100 entries saved to 'explanations.csv' file.\n"
          ]
        }
      ],
      "source": [
        "def explain_predictions(model, X_test, num_features=10):\n",
        "    explainer = lime.lime_tabular.LimeTabularExplainer(X_test, mode='classification')\n",
        "    explanations = []\n",
        "\n",
        "    for i in range(min(100, len(X_test))):  # Limit to the first 100 entries or less if the test data is smaller\n",
        "        predict_fn = lambda x: model.predict(x).astype(float)\n",
        "        exp = explainer.explain_instance(X_test[i], predict_fn, num_features=num_features)\n",
        "        explanations.append(exp.as_list())\n",
        "\n",
        "    return explanations\n",
        "\n",
        "import warnings\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "    X_test_array = X_test[:100]  # Select only the first 100 entries\n",
        "    explanations = explain_predictions(model, X_test_array)\n",
        "\n",
        "explanations_df = pd.DataFrame(explanations)\n",
        "explanations_df.to_csv('explanations.csv', index=False)\n",
        "\n",
        "print(\"Explanations for the first 100 entries saved to 'explanations.csv' file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EmsXwkep4-aa"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('ML02.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}